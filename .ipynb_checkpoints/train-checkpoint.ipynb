{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from skimage.io import imread\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "from PIL import Image\n",
    "# from utils import utils, losses, generators\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models, layers\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "EDGE_CROP = 16\n",
    "GAUSSIAN_NOISE = 0.1\n",
    "UPSAMPLE_MODE = 'SIMPLE'\n",
    "# downsampling inside the network\n",
    "NET_SCALING = (1, 1)\n",
    "# downsampling in preprocessing\n",
    "IMG_SCALING = (3, 3)\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 900\n",
    "# maximum number of steps_per_epoch in training\n",
    "MAX_TRAIN_STEPS = 5\n",
    "MAX_TRAIN_EPOCHS = 99\n",
    "AUGMENT_BRIGHTNESS = False\n",
    "\n",
    "BASE_DIR = 'airbus-ship-detection'\n",
    "TRAIN_DIR = BASE_DIR + '/train_v2/'\n",
    "TEST_DIR = BASE_DIR + '/test_v2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate through the column and determine which values in the DataFrame are filled (True) and empty (False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_ships = pd.read_csv(os.path.join(BASE_DIR, 'train_ship_segmentations_v2.csv'))\n",
    "pix = pd.notna(segmented_ships.EncodedPixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s take a look at the number of unique images containing non-empty masks and at the number of blank images and the total number of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pix.sum(), 'segmented_ships from', segmented_ships[pix].ImageId.nunique(), 'images')\n",
    "print((~pix).sum(), 'empty images from', segmented_ships.ImageId.nunique(), 'total images')\n",
    "segmented_ships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding segmentations into a numpy array. 1 = segmented regions, and 0 = background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decode ship segmentation masks, decode each mask, and then merge them into a single overall mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.uint8)\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks |= rle_decode(mask)\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_as_color(in_mask_list):\n",
    "    # Take the individual ship masks and create a color mask array for each ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.float)\n",
    "    scale = lambda x: (len(in_mask_list)+x+1) / (len(in_mask_list)*2) ## scale the heatmap image to shift\n",
    "    for i,mask in enumerate(in_mask_list):\n",
    "        if isinstance(mask, str):\n",
    "            all_masks[:,:] += scale(i) * rle_decode(mask)\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of ships in each image and see if there are any ships in the photos\n",
    "and display a histogram of the ship count in the images, and also print the maximum and average number of ships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_ships['ships'] = segmented_ships['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "unique_img_ids = segmented_ships.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "segmented_ships.drop(['ships'], axis=1, inplace=True)\n",
    "print(unique_img_ids.loc[unique_img_ids.ships>=2].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have observed that the dataset is imbalanced (150000 empty images from 192556 total images).\n",
    "\n",
    "A histogram will visually demonstrate this imbalance ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_img_ids['ships'].hist(bins=unique_img_ids['ships'].max())\n",
    "print('Max of ships : ',unique_img_ids['ships'].max())\n",
    "print('Avg of ships : ',unique_img_ids['ships'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a balanced dataset for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PER_GROUP = 4000 # determine the number of samples (masks) in each group.\n",
    "balanced_train_df = unique_img_ids.groupby('ships').apply(lambda x: x.sample(SAMPLES_PER_GROUP) if len(x) > SAMPLES_PER_GROUP else x)\n",
    "balanced_train_df['ships'].hist(bins=balanced_train_df['ships'].max()+1)\n",
    "print(balanced_train_df.shape[0], 'segmented_ships')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split the data into training and validation sets based on the balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_ids, valid_ids = train_test_split(balanced_train_df,\n",
    "                 test_size = 0.2,\n",
    "                 stratify = balanced_train_df['ships'])\n",
    "train_df = pd.merge(segmented_ships, train_ids)\n",
    "valid_df = pd.merge(segmented_ships, valid_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to efficiently load and utilize the data in batches during model training. To achieve this, we will generate data batches consisting of images and their corresponding masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_gen(in_df, batch_size = BATCH_SIZE):\n",
    "    all_batches = list(in_df.groupby('ImageId'))\n",
    "    out_rgb = []\n",
    "    out_mask = []\n",
    "    while True:\n",
    "        np.random.shuffle(all_batches)\n",
    "        for c_img_id, c_masks in all_batches:\n",
    "            rgb_path = os.path.join(TRAIN_DIR, c_img_id)\n",
    "            c_img = imread(rgb_path)\n",
    "            c_mask = np.expand_dims(masks_as_image(c_masks['EncodedPixels'].values), -1)\n",
    "            if IMG_SCALING is not None:\n",
    "                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "            out_rgb += [c_img]\n",
    "            out_mask += [c_mask]\n",
    "            if len(out_rgb)>=batch_size:\n",
    "                yield np.stack(out_rgb, 0)/255.0, np.stack(out_mask, 0).astype(np.float32)\n",
    "                out_rgb, out_mask=[], []\n",
    "\n",
    "train_gen = make_image_gen(train_df)\n",
    "train_x, train_y = next(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data has been properly loaded and prepared for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x', train_x.shape, train_x.min(), train_x.max())\n",
    "print('y', train_y.shape, train_y.min(), train_y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will check the sizes of the validation data and their corresponding masks. This will help ensure that the data has been properly prepared and is ready for use in the model validation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is used to increase the diversity of training data in order to improve the model's ability and address the issue of overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "dg_args = dict(featurewise_center = False,\n",
    "                  samplewise_center = False,\n",
    "                  rotation_range = 45,\n",
    "                  width_shift_range = 0.1,\n",
    "                  height_shift_range = 0.1,\n",
    "                  shear_range = 0.01,\n",
    "                  zoom_range = [0.9, 1.25],\n",
    "                  horizontal_flip = True,\n",
    "                  vertical_flip = True,\n",
    "                  fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last')\n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args[' brightness_range'] = [0.5, 1.5]\n",
    "image_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args.pop('brightness_range')\n",
    "label_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "def create_aug_gen(in_gen, seed = None): # generator for augmented data\n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    for in_x, in_y in in_gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        g_x = image_gen.flow(in_x,\n",
    "                             batch_size = in_x.shape[0],\n",
    "                             seed = seed,\n",
    "                             shuffle=True)\n",
    "        g_y = label_gen.flow(in_y,\n",
    "                             batch_size = in_x.shape[0],\n",
    "                             seed = seed,\n",
    "                             shuffle=True)\n",
    "\n",
    "        yield next(g_x), next(g_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_gen = create_aug_gen(train_gen) # for next package of augmented data\n",
    "t_x, t_y = next(cur_gen)\n",
    "print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t_x[37]) # augmented pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t_y[37]) # corresponding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc; gc.enable()\n",
    "\n",
    "gc.collect() # In order to accommodate augmented data that may occupy more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "\n",
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "def upsample_simple(strides):\n",
    "    return layers.UpSampling2D(strides)\n",
    "\n",
    "if UPSAMPLE_MODE=='DECONV':\n",
    "    upsample=upsample_conv\n",
    "else:\n",
    "    upsample=upsample_simple\n",
    "\n",
    "\n",
    "def unet(pretrained_weights = None, input_size = (256, 256, 3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "\n",
    "    c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = layers.Dropout(0.1)(c1)\n",
    "    c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = layers.Dropout(0.1)(c2)\n",
    "    c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = layers.Dropout(0.2)(c3)\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = layers.Dropout(0.2)(c4)\n",
    "    c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = layers.Dropout(0.3)(c5)\n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = layers.Dropout(0.2)(c6)\n",
    "    c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = layers.Dropout(0.2)(c7)\n",
    "    c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = layers.Dropout(0.1)(c8)\n",
    "    c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1], axis=3)\n",
    "    c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = layers.Dropout(0.1)(c9)\n",
    "    c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    d = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    seg_model = models.Model(inputs=[inputs], outputs=[d])\n",
    "\n",
    "    if pretrained_weights:\n",
    "        seg_model.load_weights(pretrained_weights)\n",
    "\n",
    "    return seg_model\n",
    "\n",
    "seg_model = unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):\n",
    "\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "\n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "\n",
    "    return focal_loss\n",
    "\n",
    "def DiceBCELoss(targets, inputs, smooth=1e-6):\n",
    "\n",
    "    #flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "\n",
    "    inputs = K.expand_dims(inputs)\n",
    "    targets = K.expand_dims(targets)\n",
    "\n",
    "    BCE =  binary_crossentropy(targets, inputs)\n",
    "    intersection = K.dot(K.transpose(targets), inputs)\n",
    "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
    "    Dice_BCE = BCE + dice_loss\n",
    "\n",
    "    return Dice_BCE\n",
    "\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "\n",
    "def DiceLoss(targets, inputs, smooth=1e-6):\n",
    "\n",
    "    #flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "\n",
    "    inputs = K.expand_dims(inputs)\n",
    "    targets = K.expand_dims(targets)\n",
    "\n",
    "    intersection = K.dot(K.transpose(targets), inputs)\n",
    "    dice = (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"weight_metrics/{}_weights.best.hdf5\".format('seg_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1,\n",
    "                             mode='max', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.2,\n",
    "                                   patience=3,\n",
    "                                   verbose=1, mode='max', min_delta=0.0001, cooldown=2, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor=\"val_dice_coef\",\n",
    "                      mode=\"max\",\n",
    "                      patience=15)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "def fit():\n",
    "    seg_model.compile(optimizer=Adam(1e-3, decay=1e-6), loss = FocalLoss, metrics=[dice_coef])\n",
    "\n",
    "    step_count = min(MAX_TRAIN_STEPS, train_df.shape[0]//BATCH_SIZE)\n",
    "    aug_gen = create_aug_gen(make_image_gen(train_df))\n",
    "    loss_history = [seg_model.fit(aug_gen,\n",
    "                                 steps_per_epoch=step_count,\n",
    "                                 epochs=MAX_TRAIN_EPOCHS,\n",
    "                                 validation_data=(valid_x, valid_y),\n",
    "                                 callbacks=callbacks_list,\n",
    "                                workers=1\n",
    "                                           )]\n",
    "    return loss_history\n",
    "\n",
    "while True:\n",
    "    loss_history = fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T10:14:49.300171100Z",
     "start_time": "2023-06-19T10:14:49.180172400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seg_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fullres_model \u001b[38;5;241m=\u001b[39m \u001b[43mseg_model\u001b[49m\n\u001b[0;32m      2\u001b[0m fullres_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_metrics/fullres_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seg_model' is not defined"
     ]
    }
   ],
   "source": [
    "fullres_model = seg_model\n",
    "fullres_model.save('weight_metrics/fullres_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rgb_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEST_DIR,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m00dc34840.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#'00c3db267.jpg')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(rgb_path)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m img[::IMG_SCALING[\u001b[38;5;241m0\u001b[39m], ::IMG_SCALING[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "rgb_path = os.path.join(TEST_DIR,'00dc34840.jpg')#'00c3db267.jpg')\n",
    "img = cv2.imread(rgb_path)/255\n",
    "img = img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "pred = fullres_model.predict(img)\n",
    "print(pred[pred<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pred(test_dir, img, model):\n",
    "    rgb_path = os.path.join(TEST_DIR,img)\n",
    "    img = cv2.imread(rgb_path)\n",
    "    img = img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img/255\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    pred = model.predict(img)\n",
    "    pred = np.squeeze(pred, axis=0)\n",
    "    return cv2.imread(rgb_path), pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEST_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_imgs)):\n\u001b[1;32m----> 7\u001b[0m     img, pred \u001b[38;5;241m=\u001b[39m gen_pred(\u001b[43mTEST_DIR\u001b[49m, test_imgs[i], fullres_model)\n\u001b[0;32m      8\u001b[0m     fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m      9\u001b[0m     fig\u001b[38;5;241m.\u001b[39madd_subplot(rows, columns, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TEST_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "test_imgs = ['00dc34840.jpg', '00c3db267.jpg', '00aa79c47.jpg', '00a3a9d72.jpg']\n",
    "%matplotlib inline\n",
    "\n",
    "rows = 1\n",
    "columns = 2\n",
    "for i in range(len(test_imgs)):\n",
    "    img, pred = gen_pred(TEST_DIR, test_imgs[i], fullres_model)\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Image\")\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(pred, interpolation=None)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
